"""Urban Boundary Layer Observation Data ProcessingScript name:    Spectral AnalysisPath:           ~/bin/spectral.pyDescription:    Perform spectral analysis on data collected by the flux tower atop the Marshak Building at The City College of New York."""import datetime, math, matplotlib.pyplot as plt, numpy as np, os, pandas as pd, xarray as xrimport bin.functions, bin.lidarfrom scipy.fft import fft, fftfreqfrom scipy.optimize import curve_fitdef rolling_window(data, date_range, freq, window, overlap=0.5):    '''    Generates a Pandas GroupBy object that groups data into windows of a given length.    Windows will overlap using 50% of the window size.    Parameters    ----------    data : xArray Dataset        xArray Dataset, typically containing ts_data from the Marshak flux tower.    date_range : list        List of datetime values.    freq : int or float        Frequency of provided data in Hz.    window : int or float        Size of desired rolling window, in minutes.    window : float        Size of overlap in percentage, ranging from 0 to 1.    '''        def hour_rounder(t):        # Rounds to nearest hour by adding a timedelta hour if minute >= 30        return (t.replace(second=0,                           microsecond=0,                           minute=0,                           hour=t.hour) + datetime.timedelta(hours=t.minute//30))    # Initialize DataFrame and populate with xArray data    df = data.to_dataframe()    df['time'] = df.index.get_level_values('time')    # Define date range to span given range without missing timestamps.    # This will be used to backfill missing times with nans.    date_range = pd.date_range(start=hour_rounder(df['time'].iloc[0]),                                end=hour_rounder(df['time'].iloc[-1]),                                freq='{0:1f}S'.format(1/freq))    # Define size of window given input parameters    window_size = window * 60 * freq    # Change index to allow for re-indexing using new date range    df = df.set_index('time', drop=False)    df['time'] = df['time'].dt.round('{0:1f}S'.format(1/freq))    df = df[~df.index.duplicated()].reindex(df['time'], fill_value=np.nan)    # Ensure overlap value is valid. If not, return nothing.    if not 0 <= overlap <= 1:        return None    else:        # Generate list of windows through iteration.         # Iteration step size is determined by the overlap set        dfs = []        for i in np.arange(0, len(df), math.floor((1 - overlap) * window_size))[:-1]:            dfs.append(df.iloc[i:(i + window_size)])        return dfsdef processor(data, site='MANH', height=np.nan, intv='30T', param='w', plot=False, metadata=None, normalization=None):    '''    Perform spectral analysis on a DataFrame given a time interval and parameter.    Parameters    ----------    data : xArray Dataset        xArray Dataset output from ~/bin/lidar.py    intv : str        Time interval to chunk DataFrame into. See https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects for reference on how to format this string. Default value of '30T', or 30 minutes, provided.    param : str        Column header string of parameter to analyze.    plot : bool        Determines whether or not a plot will be generated for this data.    metadata : str        Metadata to be added to the plot.    normalization : list or None        Mean horizontal wind speed (U) and zonal wind speed (u) to be used for normalization of spectra.    Returns    -------    freq : NumPy array        Array of averaged spectral analysis frequencies.    arr : NumPy array        Array of averaged spectral analysis power densities.    '''        # Override for troubleshooting and testing purposes    intv = '1M'        # Extract Pandas DataFrame from xArray data.    if type(data) != pd.core.frame.DataFrame:        data = data.sel(site=site, height=height).to_dataframe()    # Group data based on given interval    groups = data.groupby(pd.Grouper(freq=intv))    # [print(key) for key, group in groups]    # Define size of empty Fourier matrix    size = math.ceil(max([len(group) for key, group in groups]) / 2)    # Initialize lists for future averaging for frequency and Fourier transform data    freqs, arrs, N, T = np.empty(((size, 0)), float), np.empty((size, 0), float), np.nan, np.nan    # Iterate through groups and perform transforms    for key, group in groups:        # Filter out nans from data        group_data = group[param].to_numpy()        group_data = group_data[~np.isnan(group_data)]        # Initialize nan arrays with uniform size. Necessary for averaging.        freq, arr = np.full(size, np.nan), np.full(size, np.nan)        # Define number of samples in group        N = len(group_data)        # Define average time difference between each sample        T = np.nanmean(np.diff(group.index)) / np.timedelta64(1, 's')        # Calculate frequency domain        freq_ = fftfreq(N, T)        # Place frequencies into nan array, filter out negatives        freq[0:len(freq_[freq_>=0])] = freq_[freq_>=0]        # Calculate fast Fourier transform array for the given parameter        arr_ = np.abs(fft(group_data)[freq_>=0])**2        # Place frequencies into nan array        arr[0:len(arr_)] = arr_        # Append data to respective lists        freqs = np.column_stack((freqs, freq))        arrs = np.column_stack((arrs, arr))            # Obtain averaged data    freq = np.nanmean(freqs, axis=1)    arr = np.nanmean(arrs, axis=1)         # Normalization    if normalization is not None:        U, u = normalization        U, u = U.sel(height=height), u.sel(height=height)        # Clip normalization data        times = min(data.index.values), max(data.index.values)        U, u = U.sel(time=slice(min(data.index.values), max(data.index.values))), u.sel(time=slice(min(data.index.values), max(data.index.values)))        # Select appropriate height        freq_div = np.ceil(freq.shape[0] // U.values.shape[0])        arr_div = np.ceil(arr.shape[0] // u.values.shape[0])        U = np.repeat(U.values, freq_div)        u = np.repeat(u.values, arr_div)        freq = freq[0:len(U)] * height / U        arr = freq * arr[0:len(u)] / u        # Define number of frequency bands    N_bands = 32    # Define frequency bands    freq_bands = np.logspace(np.log10(sorted(freq)[1]), np.log10(np.nanmax(freq)), N_bands)    freq_bands = np.insert(freq_bands, 0, 0)    # Average data over defined frequency bands    freq_avgd = [np.nanmean(freq[(freq >= freq_bands[i]) & (freq < freq_bands[i+1])]) for i in np.arange(0, len(freq_bands)-1)]    arr_avgd = [np.nanmean(arr[(freq >= freq_bands[i]) & (freq < freq_bands[i+1])]) for i in np.arange(0, len(freq_bands)-1)]        # Test plot    if plot:        fig, ax = plt.subplots(dpi=300)        im = ax.loglog(freq, arr, alpha=0.2, lw=0)        im_avgd = ax.loglog(freq_avgd, arr_avgd, marker='o', lw=0)        fig.suptitle('Fast Fourier Transform, {0}'.format(param))        ax.set_title('{0} to {1}, $\zeta$ = {2}'.format(data.index[0], data.index[-1], metadata))        ax.set_ylabel('Spectral energy density')        ax.set_xlabel('Frequency (Hz)')        ax.legend(['Raw', 'Frequency-avgd'])        return (freq, arr), (freq_avgd, arr_avgd)def proc(height=200):    '''    Perform spectral analysis for a given set of lidar and flux tower data.    Note that data is pre-selected (lidar data from 2021-07-31 to 08-31, flux tower data from 2021-07-31 to 08-10)    Parameters    ----------    height : int, optional        Height at which the lidar data will be analyzed. The default is 200.    Returns    -------    spectra : dict        Dictionary containing stability-grouped lists containing stability grouping as the key, and the a list of normalized frequency, normalized spectra, frequency-averaged frequencies, and frequency-averaged spectra as the value.    '''        ''' Pull flux tower data from atop Marshak. '''    # List files containing relevant data    files = ['/Volumes/UBL Data/data/flux/MANH/TOA5_10560_s20210720_e20210802_flux.dat', '/Volumes/UBL Data/data/flux/MANH/TOA5_10560_s20210802_e20210818_flux.dat']    # Initialize list to hold DataFrames that will later be concatenated    ts_data = []    # Iterate through files to read the .dat files    for file in files:         # Read in the .dat file. Keep 2nd row for column headings, skip the following rows (unnecessary metadata), replace all bad values with np.nan        temp = pd.read_table(file, sep=',', header=[1], skiprows=[2, 3], na_values='NAN')        ts_data.append(temp)    # Concatenate the .dat file DataFrames    ts_data = pd.concat(ts_data)    # Adjust time index for daylight savings time (UTC-4)    ts_data['TIMESTAMP'] = pd.to_datetime(ts_data['TIMESTAMP']) - datetime.timedelta(hours=4)    # Define Marshak building height.    # See https://data.cityofnewyork.us/Housing-Development/Building-Footprints/nqwf-w8eh for building height data    ts_data['z'] = 50.6    # Calculate Obukhov length. See Stull, 1988, Eq. 5.7c.    ts_data['L'] = -ts_data['Ts_Avg'].to_numpy().flatten()*ts_data['u_star'].to_numpy().flatten()/(0.4*9.81*ts_data['Ts_Uz_cov'].to_numpy().flatten())    # Calculate atmospheric stability parameter.    ts_data['zeta'] = ts_data['z']/ts_data['L']    # Calculate mean horizontal wind velocity. To be used for normalization.    ts_data['U'] = np.sqrt(ts_data['Ux_Avg']**2 + ts_data['Uy_Avg']**2)        ''' Pull lidar data. '''    # Get lidar data from the August lidar file dedicated for spectral analysis    lidar_data = xr.open_dataset('/Volumes/UBL Data/data/storage/lidar/lidar_data_2021-08_spectral.nc').sel(site='MANH', height=height)    # Select lidar data for dates of interest and convert to DataFrame    df = lidar_data.sel(time=slice('2021-07-31', '2021-08-10')).to_dataframe()    # Remove MultiIndex to allow for time and height indices to become columns    df = df.reset_index()    # Group DataFrame into 30-minute intervals. This allows for matching with 30-minute-averaged ts_data    df_grouped = df.groupby(pd.Grouper(key='time', freq='30T'))    # Get averaged lidar data sampling interval    dt = np.nanmean(np.diff(df['time'])) / np.timedelta64(1, 's')        ''' Match the lidar and flux tower data to permit grouping by stability. '''    # Initialize list of modified DataFrames to be concatenated.    dfs = []    # For the 30-minute binned DataFrames, add columns with data from the flux tower data.    for label, data in df_grouped:        # Match lidar and flux tower data by timestamp        match = ts_data.loc[ts_data['TIMESTAMP'] == label]        # Filter out necessary columns for spectra normalization        match = match[['z', 'U', 'u_star', 'zeta']]        # If the matched DataFrame column is empty, fill with np.nan        for col in match.columns:            if match[col].empty:                match[col] = [np.nan]        # Append the matching value to the DataFrame as a column for each parameter needed for normalization.        data['z'] = np.repeat(match['z'].values, len(data))        data['U'] = np.repeat(match['U'].values, len(data))        data['u_star'] = np.repeat(match['u_star'].values, len(data))        data['zeta'] = np.repeat(match['zeta'].values, len(data))        # Append the matched DataFrame to the list of DataFrames        dfs.append(data)    # Concatenate the matched DataFrames    dfs = pd.concat(dfs)    # Define stability groups    bins = [-np.inf, -2, -1, -0.5, 0, 0.5, 1, 2, np.inf]    # Group the matched DataFrames by stability classification    dfs_grouped_zeta = dfs.groupby(pd.cut(dfs['zeta'], bins))        ''' Perform spectral analysis for each stability grouping. '''    # Initialize dictionary to hold all grouped spectra    spectra = {}    # Iterate over every stability grouping    for label, data in dfs_grouped_zeta:        # Filter out all nan data        data = data[data['w'].notna()]        # Number of sample points         N = len(data['w'])        # Get averaged lidar data sampling frequency        T = 1/dt        # Get fast Fourier transform frequencies (negative frequencies filtered out)        x = fftfreq(N, T)        # Get normalized frequencies.         # Indexing changes based on whether N is even or odd, see https://docs.scipy.org/doc/scipy/reference/tutorial/fft.html        if N % 2 == 0:            x_norm = (x * height / data['U'])[:N//2]        else:            x_norm = (x * height / data['U'])[:(N-1)//2]        # Perform fast Fourier transform with normalized data        y_norm = np.abs(x * fft(data['w'].values) / (data['U']**2).values)[:N//2]                ''' Frequency band averaging. '''        # Define number of frequency bands        N_bands = 32        # Define frequency bands        freq_bands = np.logspace(np.log10(sorted(x_norm)[1]), np.log10(np.nanmax(x_norm)), N_bands)        # Prepend a 0 to prevent a nonzero first element        # freq_bands = np.insert(freq_bands, 0, 0)        # Average frequency over defined frequency bands        freq_avgd = [np.nanmean(x_norm[(x_norm >= freq_bands[i]) & (x_norm < freq_bands[i+1])]) for i in np.arange(0, len(freq_bands)-1)]        # Average spectra over defined frequency bands        arr_avgd = [np.nanmean(y_norm[(x_norm >= freq_bands[i]) & (x_norm < freq_bands[i+1])]) for i in np.arange(0, len(freq_bands)-1)]                spectra[label] = [x_norm, y_norm, freq_avgd, arr_avgd]        return spectra, dfsdef plotter(data, heights):    '''    Plot a list of spectral data for a series of heights.    Parameters    ----------    data : list        List of dictionaries containing spectral data from the proc() function.    heights : list        List of heights at which spectra were obtained for.    '''        # Check to make sure that the data input matches the heights analyzed    if len(data) != len(heights):        print('Check your inputs, the data list and height list should be equal...')        # Curve fitting - Kaimal spectrum per Larsen (2016) as written in Cheynet (2017)    def func(x, a, b, c, d):        return a*x**(-2/3) + b*x/(1+c*x)**(5/3) + d*x**(-2)        # Curve fitting - implementation    def kaimal_spectrum(x, y):        # Filter out nans from both arrays        x_ = x[~np.isnan(x) & ~np.isnan(y)]        y_ = y[~np.isnan(x) & ~np.isnan(y)]        # Get curve fitting metadata        popt, pcov = curve_fit(func, x_, y_, maxfev=100000)        return x_, y_, popt, pcov        # Define figure parameters    ncols = 2    nrows = len(data[0]) // ncols    # Initialize figure    fig, ax = plt.subplots(figsize=(6, 12), dpi=300, nrows=nrows, ncols=ncols)    # Iterate over subplot to plot.    # Outer loop iterates over stability groups.    for i, ax in enumerate(ax.reshape(-1)):        # Obtain name of stability group        key = list(data[0].keys())[i]        # Iterate over heights        for j in range(0, len(data)):            # Obtain frequency-averaged frequency bins and spectra            x, y = data[j][key][2], data[j][key][3]            # Plot the data            im = ax.loglog(x, y, marker='o', markersize=4, lw=0, label='{0} m'.format(heights[j]))            # Plot spectral reference data for lowest level            # if j == 0:                # x_, y_, popt, pcov = kaimal_spectrum(np.array(x), np.array(y))                # im_kaimal = ax.loglog(x_, func(x_, *popt), color='k', linestyle='--', lw=2)                        ax.grid()            # For u* normalization            # ax.set_xlim([10e-5, 10e2])            # ax.set_ylim([10e1, 10e4])            # For w normalization            # ax.set_xlim([10e-5, 10e2])            # ax.set_ylim([10e-1, 10e2])            ax.set_xlabel('$f z / \overline{U}$')            ax.set_ylabel('$f S_w / \overline{U}^2$')            ax.set_title('$\zeta$ = {0}'.format(key))        ax.legend()            fig.tight_layout()if __name__ == '__main__':    spectra, heights = [], [200]    for height in heights:        temp, df = proc(height=height)        spectra.append(temp)    plotter(spectra, heights)            